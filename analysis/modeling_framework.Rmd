---
title: "Modeling framework - Dirichlet Adaptive shrinkage and smoothing"
author: "Kushal K Dey"
date: "4/28/2018"
output: html_document
---

## Dirichlet Adaptive Shrinkage (*dash*)

Assume that there are \(L\) constituents in the compositional mix. \(L\) equals \(4\) (corresponding to A,C, G and T bases) for the DNa sequence motif data and \(20\) corresponding to the amino acids for the protein sequence data.

Suppose there are $L$ categories and $n$ positions. We model these compositional counts vectors as follows

$$ (c_{n1}, c_{n2}, \cdots, c_{nL}) \sim Mult \left ( c_{n+} : p_{n1}, p_{n2}, \cdots, p_{nL} \right ) $$

where \(c_{n+}\) is the total frequency of the different constituents of the compositional data observed for the \(n\) th base. \(p_{nl}\) here represents the compositional probability for the $l$ th base in the position $n$. We have

$$ p_{nl} >= 0 \hspace {1 cm} \sum_{l=1}^{L} p_{nl} = 1 $$

We choose the Dirichlet prior distribution on the compositional probability vector \((p_{n1}, p_{n2}, \cdots, p_{nL})\). In order to perform adaptive shrinkage
$\left(\mu_{1}, \mu_{2}, \cdots, \mu_{L} \right)$, but with varying amounts of concentration, which need to be estimated along with the unknown mixture proportions from the data. 

$$\left ( p_{n1}, p_{n2}, \cdots, p_{nL} \right ) : = \sum_{k=1}^{K} \pi_{k} Dir \left (\alpha_{k} \mu_{1}, \alpha_{k} \mu_{2}, \cdots, \alpha_{k} \mu_{L} \right ) \hspace {0.5 cm} \alpha_{k} > 0 \hspace{0.5 cm} \sum_{l=1}^{L} \mu_{l} = 1 
$$

We assume a prior of \(\pi_{k}\) to be Dirichlet

$$ f(\pi) : = \prod_{k=1}^{K} {\pi_{k}}^{\lambda_{k}-1}  $$

### Default parameters

We choose a default set of \(\alpha_{k}\) to be \((Inf, 100, 50, 20, 10, 2, 1, 0.1, 0.01)\). In this case \(\alpha_{k}=Inf\) corresponds essentially to point mass at the prior mean or background mean \(\mu_{1}, \mu_{2}, \cdots, \mu_{L}\), and then the subsequent choices of \(\alpha_{k}\) have lower degree of concentration. $\alpha_{k} = 1$ corresponds to the most uniform scenario, whereas \(\alpha_{k} < 1\) correspond to cases with probability masses at the edges of the simplex but with the mean at the prior mean. The latter components would direct the points close to the corners towards the corners and away from the center, resulting in clearer separation of the points closer to the mean with the ones away from it.

We choose the default prior amount of shrinkage of \(\pi_{k}\), namely \(\lambda_{k}\) to be \(\left( 1, 1, 1, 1, \cdots, 1 \right )\). The user may want to increase the weight on the first term (corresponding to \(\alpha_{k} = Inf \)  ) to enforce stronger shrinkage. 

## Adaptive smoothing using *dash*  (*dash-m*)

Consider a process 

$$  Y_t \overset{iid}{\sim} Poi \left ( \mu_t  \right )  $$

where $t=1, 2, , \cdots T$ be a sequence of base positions (analogous to time). 
Suppose we are interested in estimating $\mu$. We assume that the function $\mu$ is smooth across time and the noise distribution is Poisson around $\mu$. Once can then apply Poisson multiscale models to estimate the $\mu$.

We have 

$$  Y_1 + Y_2 \sim Poi \left ( \mu_1 + \mu_2  \right ) $$

$$  Y_1 | Y_1 + Y_2 \sim Bin \left (Y_1 + Y_2, \frac{\mu_1}{\mu_1 + \mu_2} \right )$$

We introduce the notation $v_{i:j}$ to denote for a vector $v$, the sum 
$\sum_{t=i}^{j} v_{t}$. for $T=4$, using the new notation,  we can write 

$$ Y_{1:2} \sim Poi \left ( \mu_{1:2}  \right ) $$

$$ Y_{1:4} \sim Poi \left ( \mu_{1:4} \right ) $$

$$  Y_{1} | Y_{1:2} \sim Bin \left ( Y_{1:2}, \frac{\mu_{1}}{\mu_{1:2}} \right ) $$

$$  Y_{3} | Y_{3:4} \sim Bin \left ( Y_{3:4}, \frac{\mu_{3}}{\mu_{3:4}} \right ) $$


$$ Y_{1:2} | Y_{1:4} \sim Bin \left ( Y_{1:4}, \frac{\mu_{1:2}}{\mu_{1:4}} \right ) $$


Together these models are independent decomposition of the $Y_j \sim Poi (\mu_j)$ process. This decomposition also creates an analog set of parameters corresponding to $\mu = (\mu_1, \mu_2, \mu_3, \mu_4)$ in the form of 
$p = (p_1, p_2, p_3) = ( \mu_{1} / \mu_{1:2}, \mu_{3} / \mu_{3:4}, \mu_{1:2} / \mu_{1:4})$ and the total intensity $\mu_{1:4}$. 

The vector of parameters $p$ can be treated as the proportional message flow along the multiscale tree on the counts data, and in our model, are treated as independently generated quantities from a mixture of Beta distributions. We assume a separate prior for separate level of the Polya or multiresolution tree. For $T=4$, there are two levels, the lower level 
$l=2$ corresponding to $p_1$ and $p_2$ and the upper level $l=1$, with $p_3$.

$$  p_{1}, p_{2} = (p^{(2)}_{1}, p^{(2)}_{2}) \sim \sum_{k=1}^{K} \pi_{1k} Beta (\alpha_{k}, \alpha_{k}) $$
$$  p_{3} = p^{(1)}_{1} \sim \sum_{k=1}^{K} \pi_{2k} Beta (\alpha_{k}, \alpha_{k}) $$

The $\alpha_{k}$ concentration values are fixed a priori and we estimate the parameters $\pi_{1.}$ and $\pi_{2.}$.

The above example was for $T=2^2$ which resulted in 3 parameters of $p$ to estimate. In case when $T= 2^L$, we have 
$T-1$ parameter values of $p$ to estimate. These $p$ values can be decomposed into $L$ resolution levels like we had
2 resolution level in the above example. Then for the $l$th resolution level, we have 

$$ (p^{(l)}_{1},  p^{(l)}_{2}, \cdots, p^{(l)}_{2^{l-1}}) \sim \sum_{k=1}^{K} \pi_{lk} Beta (\alpha_{k}, \alpha_{k}) $$

We for a fully data driven smoothing, we assume a uniform prior  on $\pi$

$$ \left (\pi_{l1}, \pi_{l2}, \cdots, \pi_{lK} \right) \sim Dir \left(1, 1, \cdots, 1 \right ) $$

Again we take the default $\alpha_k$ to be $\left(Inf, 100, 50, 20, 10, 2, 1, 0.005 \right)$. The $0.005$ concentration 
is taken to push the proportions to the edges if they are very different, whereas Inf concentration would correspond to
a point mass function in standard shrinkage models. We provide the flexibility to remove the Inf and/or the $<1$ concentrations. The background mean of the Beta prior is always 0.5, pushing 
consecutive points towards each other, but the level of shrinkage is determined by the weights on the different concentration Beta components. 

In practical implementation, we usually create a TI table to expand the number of message flow proportions at each level.


